## Welcome to TAIL!

<center>
<img src="img/TAIL.png" alt="TAIL" width="400">
</center>
<center>
Automatic, Easy and Realistic tool for LLM Evaluation

</center>
<center>
<a href="getting-started/" class="btn btn-primary">Getting Started</a>
<a href="userguide/" class="btn btn-primary">User Guide</a>
</center>

<div class="pt-2 pb-4 px-4 my-4 bg-body-tertiary rounded-3">
<h3 class="text-center">Features</h3>

<div class="row">
  <div class="col-sm-6">
    <div class="card mb-4">
      <div class="card-body">
        <h3 class="card-title">Easy to customize</h3>
        <p class="card-text"> 
            TAIL helps you generate benchmarks on your own documents (Patents, Papers, Financial Reports, anything you are interested in). It allows you to create test examples of any context length and questions at any depth you desire.
        </p>
      </div>
    </div>
  </div>
  <div class="col-sm-6">
    <div class="card mb-4">
      <div class="card-body">
        <h3 class="card-title">Realistic and natural</h3>
        <p class="card-text">
            Unlike the 
            <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">needle-in-a-haystack</a> test, TAIL generate questions based on information from your own document, instead of inserting a piece of new infomation, making the benchmark more realistic and natural.
        </p>
      </div>
    </div>
  </div>
</div>

<div class="row">
  <div class="col-sm-6">
    <div class="card">
      <div class="card-body">
        <h3 class="card-title">Quality assured</h3>
        <p class="card-text">
            TAIL utilizes multiple quality assurance measures, including RAG-based filtering and rigorous quality checks, to eliminate subpar QAs and deliver a high-caliber benchmark.
        </p>
      </div>
    </div>
  </div>
  <div class="col-sm-6">
    <div class="card">
      <div class="card-body">
        <h3 class="card-title">Ready-to-use</h3>
        <p class="card-text">
            TAIL integrates an out-of-the-box evaluation module that enables users to easily evaluate commercial LLMs via API calls and open-source LLMs via <a href="https://docs.vllm.ai/en/latest/">vLLM</a> on the generated benchmarks.
        </p>
      </div>
    </div>
  </div>
</div>
</div>
<!-- TAIL is a toolkit for automatically creating realistic evaluation
benchmarks and assessing the performance
of long-context LLMs. With TAIL, users
can customize the generation of natural and reliable QAs at specific depths to construct a long-context,
document-grounded QA benchmark and obtain
visualized performance metrics of evaluated
models. TAIL has the advantage of requiring minimal human annotation and generating natural questions based on user-provided
long-context documents.  -->