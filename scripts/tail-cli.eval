#!/usr/bin/env python

import argparse,os
from openai import OpenAI  
from tail_test.test_llm_performance import test_llm_performance
from tail_test.visualize import visualize


if __name__ == '__main__':
    usage_text = """\
tail-cli.eval [--QA_save_path <path>] [--test_model_name <name>] 
                      [--test_depth_list <value>] [--test_doc_length <value>] 
                      [--test_result_save_dir <path>]

Options:
  --QA_save_path <path>          Path to the saved QA dataset.
  --test_model_name <name>       Test model name (default: gpt-4o).
  --test_depth_list <value>      Depths you want to test. Multiple values can be provided (default: 30 70).
  --test_doc_length <value>      Token lengths you want to test. Multiple values can be provided (default: 8000).
  --test_result_save_dir <path>  Path to save the test results and visualizations.
"""

    parser = argparse.ArgumentParser(
        description="",
        usage=usage_text,
        formatter_class=argparse.RawTextHelpFormatter)

    parser.add_argument('--QA_save_path', type=str, default = "/data/QA.json", help='path to save the QA dataset')
    parser.add_argument('--test_model_name', type=str, default="gpt-4o", help='test model name')
    parser.add_argument('--test_depth_list', type=int, nargs='+', default=[30,70], help='what depths you want to test')
    parser.add_argument('--test_doc_length', type=int, nargs='+', default=[8000], help='what token lengths you want to test')
    parser.add_argument('--test_result_save_dir', type=str, help='path to save the test results and visualizations')
    
    args = parser.parse_args()

    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
    test_llm_performance(args,client)
    visualize(args)

