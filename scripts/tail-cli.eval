#!/usr/bin/env python

import argparse,os
from openai import OpenAI  
from utils.benchmark_generation import gen_benchmark
from utils.test_llm_performance import test_llm_performance
from utils.visualize import visualize
from utils.needle_gen import needle_gen

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('--QA_save_path', type=str, default = "/data/QA.json", help='path to save the QA dataset')
    parser.add_argument('--test_model_name', type=str, default="gpt-4o", help='test model name')
    parser.add_argument('--test_depth_list', type=int, nargs='+', default=[45], help='what depths you want to test')
    parser.add_argument('--test_doc_length', type=int, nargs='+', default=[8000, 40000, 72000], help='what token lengths you want to test')
    parser.add_argument('--test_result_save_dir', type=str, help='path to save the test results and visualizations')
    
    args = parser.parse_args()

    client = OpenAI(base_url="https://yanlp.zeabur.app/v1", api_key=os.environ["OPENAI_API_KEY"])

    test_llm_performance(args,client,client2)
    visualize(args)

